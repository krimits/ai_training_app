import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_blobs
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import time

st.set_page_config(layout='wide', page_title='AI Training â€” Î Î»Î®ÏÎ·Ï‚ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·', page_icon='ğŸ¤–')

def section_title(t): 
    st.markdown(f'## ğŸ“Œ {t}')

def subsection_title(t):
    st.markdown(f'### {t}')

def show_quiz(q):
    st.write('**Î•ÏÏÏ„Î·ÏƒÎ·:**', q['question'])
    if q['type'] == 'mcq':
        choice = st.radio('Î•Ï€Î¹Î»Î¿Î³Î­Ï‚:', q['options'], key=q['id'], label_visibility='visible')
        if st.button('Î¥Ï€Î¿Î²Î¿Î»Î® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚', key=q['id']+'_btn'):
            if choice == q['answer']:
                st.success('âœ… Î£Ï‰ÏƒÏ„ÏŒ! ' + q.get('explain',''))
                st.balloons()
            else:
                st.error('âŒ Î›Î¬Î¸Î¿Ï‚. ' + q.get('explain',''))
    elif q['type'] == 'tf':
        choice = st.radio('Î•Ï€Î¹Î»Î­Î¾Ï„Îµ:', ['Î£Ï‰ÏƒÏ„ÏŒ','Î›Î¬Î¸Î¿Ï‚'], key=q['id'], label_visibility='visible')
        if st.button('Î¥Ï€Î¿Î²Î¿Î»Î®', key=q['id']+'_btn'):
            ans = 'Î£Ï‰ÏƒÏ„ÏŒ' if q['answer']==True else 'Î›Î¬Î¸Î¿Ï‚'
            if choice==ans:
                st.success('âœ… Î£Ï‰ÏƒÏ„ÏŒ! ' + q.get('explain',''))
                st.balloons()
            else:
                st.error('âŒ Î›Î¬Î¸Î¿Ï‚. ' + q.get('explain',''))

st.title('ğŸ¤– AI Training â€” Î Î»Î®ÏÎ·Ï‚ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏƒÏ„Î·Î½ Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·')
st.markdown('### Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ Î¤ÎµÏ‡Î½Î·Ï„Î®Ï‚ ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·Ï‚ ÎºÎ±Î¹ ChatGPT ÏƒÎµ ÎšÏÎ¯ÏƒÎ¹Î¼Î¿Ï…Ï‚ Î¤Î¿Î¼ÎµÎ¯Ï‚')
st.markdown('---')

tabs = st.tabs(['ğŸ“š Î ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿','ğŸ Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Python','ğŸ”¬ Î•Î¾Î¿Î¼Î¿Î¹ÏÏƒÎµÎ¹Ï‚ AI','âœ… ÎšÎ¿Ï…Î¯Î¶','ğŸ’¡ Î”Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ­Ï‚ Î‘ÏƒÎºÎ®ÏƒÎµÎ¹Ï‚','ğŸ“– Î ÏŒÏÎ¿Î¹'])

with tabs[0]:
    section_title('1.1 Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® â€” Î¤Î¹ ÎµÎ¯Î½Î±Î¹ Î· Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·')
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        #### ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ±Î¹ Î“ÎµÎ½Î¹ÎºÎ¬
        Î— **Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î· (Artificial Intelligence - AI)** ÎµÎ¯Î½Î±Î¹ Î¿ ÎºÎ»Î¬Î´Î¿Ï‚ Ï„Î·Ï‚ ÎµÏ€Î¹ÏƒÏ„Î®Î¼Î·Ï‚ Ï„Ï‰Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„ÏÎ½ 
        Ï€Î¿Ï… Î±ÏƒÏ‡Î¿Î»ÎµÎ¯Ï„Î±Î¹ Î¼Îµ Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ…ÏƒÏ„Î·Î¼Î¬Ï„Ï‰Î½ Î¹ÎºÎ±Î½ÏÎ½ Î½Î± ÎµÎºÏ„ÎµÎ»Î¿ÏÎ½ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ Ï€Î¿Ï… Ï€Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÎ¬ Î±Ï€Î±Î¹Ï„Î¿ÏÎ½ 
        Î±Î½Î¸ÏÏÏ€Î¹Î½Î· Î½Î¿Î·Î¼Î¿ÏƒÏÎ½Î·.
        
        **Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚:**
        - ğŸ§  **Machine Learning (ML)**: ÎœÎ·Ï‡Î±Î½Î­Ï‚ Ï€Î¿Ï… Î¼Î±Î¸Î±Î¯Î½Î¿Ï…Î½ Î±Ï€ÏŒ Î´ÎµÎ´Î¿Î¼Î­Î½Î±
        - ğŸŒ **Deep Learning**: ÎÎµÏ…ÏÏ‰Î½Î¹ÎºÎ¬ Î´Î¯ÎºÏ„Ï…Î± Î¼Îµ Ï€Î¿Î»Î»Î¬ ÎµÏ€Î¯Ï€ÎµÎ´Î±  
        - ğŸ’¬ **Natural Language Processing (NLP)**: Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï†Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î³Î»ÏÏƒÏƒÎ±Ï‚
        - ğŸ‘ï¸ **Computer Vision**: ÎŒÏÎ±ÏƒÎ· Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„ÏÎ½
        - ğŸ¤– **Robotics**: Î¡Î¿Î¼Ï€Î¿Ï„Î¹ÎºÎ® ÎºÎ±Î¹ Î±Ï…Ï„Î¿Î½Î¿Î¼Î¯Î±
        """)
        
    with col2:
        st.info("""
        **ğŸ’¡ Î“Î½Ï‰ÏÎ¯Î¶Î±Ï„Îµ ÏŒÏ„Î¹:**
        
        Î— AI Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÏƒÎµ:
        - Smartphones (Siri, Alexa)
        - Î‘Ï…Ï„ÏŒÎ½Î¿Î¼Î± Î¿Ï‡Î®Î¼Î±Ï„Î±
        - Î™Î±Ï„ÏÎ¹ÎºÎ­Ï‚ Î´Î¹Î±Î³Î½ÏÏƒÎµÎ¹Ï‚
        - ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚
        - Î•Î¾Ï…Ï€Î·ÏÎ­Ï„Î·ÏƒÎ· Ï€ÎµÎ»Î±Ï„ÏÎ½
        """)
    
    st.markdown('---')
    section_title('1.2 ÎšÏÏÎ¹Î± Î”Î¿Î¼Î¹ÎºÎ¬ Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Ï„Î·Ï‚ Î¤ÎµÏ‡Î½Î·Ï„Î®Ï‚ ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·Ï‚')
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.markdown("""
        ### ğŸ“Š Î”ÎµÎ´Î¿Î¼Î­Î½Î±
        Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÎµÎ¯Î½Î±Î¹ Î· Î²Î¬ÏƒÎ· ÎºÎ¬Î¸Îµ AI ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚
        - Big Data
        - Î Î¿Î¹ÏŒÏ„Î·Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        - Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±
        """)
    with col2:
        st.markdown("""
        ### âš™ï¸ Î‘Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Î¹
        ÎœÎ±Î¸Î·Î¼Î±Ï„Î¹ÎºÎ­Ï‚ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Î¼ÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯ÎµÏ‚
        - Supervised Learning
        - Unsupervised Learning
        - Reinforcement Learning
        """)
    with col3:
        st.markdown("""
        ### ğŸ¯ ÎœÎ¿Î½Ï„Î­Î»Î±
        Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î± ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î±
        - Neural Networks
        - Decision Trees
        - SVM
        """)
    with col4:
        st.markdown("""
        ### ğŸ’» Î¥Ï€Î¿Î´Î¿Î¼Î­Ï‚
        Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ® Î¹ÏƒÏ‡ÏÏ‚
        - GPU/TPU
        - Cloud Computing
        - Frameworks
        """)
    
    st.markdown('---')
    section_title('1.3 Î’Î±ÏƒÎ¹ÎºÎ¬ Î™ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ Î•Ï€Î¹Ï„ÎµÏÎ³Î¼Î±Ï„Î± ÏƒÏ„Î¿ Î§ÏÏÎ¿ Ï„Î·Ï‚ Î¤ÎµÏ‡Î½Î·Ï„Î®Ï‚ ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·Ï‚')
    
    timeline_data = {
        'ÎˆÏ„Î¿Ï‚': ['1950', '1956', '1997', '2011', '2012', '2016', '2018', '2020', '2022'],
        'Î“ÎµÎ³Î¿Î½ÏŒÏ‚': [
            'Turing Test',
            'Dartmouth Conference - Î“Î­Î½Î½Î·ÏƒÎ· Ï„Î·Ï‚ AI',
            'Deep Blue Î½Î¹ÎºÎ¬ Ï„Î¿Î½ Kasparov',
            'Watson Î½Î¹ÎºÎ¬ ÏƒÏ„Î¿ Jeopardy',
            'AlexNet - Deep Learning Revolution',
            'AlphaGo Î½Î¹ÎºÎ¬ Ï„Î¿Î½ Lee Sedol',
            'GPT-2 - Î“Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î±',
            'GPT-3 - 175 billion Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹',
            'ChatGPT - ÎœÎ±Î¶Î¹ÎºÎ® Ï…Î¹Î¿Î¸Î­Ï„Î·ÏƒÎ· AI'
        ]
    }
    
    df_timeline = pd.DataFrame(timeline_data)
    st.dataframe(df_timeline, use_container_width=True, hide_index=True)
    
    with st.expander('ğŸ“– Î”Î¹Î±Î²Î¬ÏƒÏ„Îµ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î³Î¹Î± Ï„Î·Î½ Î¹ÏƒÏ„Î¿ÏÎ¯Î±'):
        st.markdown("""
        **1950 - Alan Turing**: Î ÏÏŒÏ„ÎµÎ¹Î½Îµ Ï„Î¿ Turing Test Î³Î¹Î± Î½Î± Î±Î¾Î¹Î¿Î»Î¿Î³Î®ÏƒÎµÎ¹ Î±Î½ Î¼Î¹Î± Î¼Î·Ï‡Î±Î½Î® Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÏƒÎºÎ­Ï†Ï„ÎµÏ„Î±Î¹.
        
        **1956 - Dartmouth Conference**: ÎŸ John McCarthy ÎµÏ€Î¹Î½ÏŒÎ·ÏƒÎµ Ï„Î¿Î½ ÏŒÏÎ¿ "Artificial Intelligence".
        
        **1997**: Î¤Î¿ Deep Blue Ï„Î·Ï‚ IBM Î½Î¯ÎºÎ·ÏƒÎµ Ï„Î¿Î½ Ï€Î±Î³ÎºÏŒÏƒÎ¼Î¹Î¿ Ï€ÏÏ‰Ï„Î±Î¸Î»Î·Ï„Î® ÏƒÎºÎ±ÎºÎ¹Î¿Ï Garry Kasparov.
        
        **2012**: Î— ÎµÏ€Î±Î½Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î¿Ï… Deep Learning Î¼Îµ Ï„Î¿ AlexNet Ï€Î¿Ï… Î½Î¯ÎºÎ·ÏƒÎµ ÏƒÏ„Î¿ ImageNet competition.
        
        **2022-ÏƒÎ®Î¼ÎµÏÎ±**: Î— ÎµÏ€Î¿Ï‡Î® Ï„Ï‰Î½ Large Language Models Î¼Îµ ChatGPT Î½Î± Ï†Î­ÏÎ½ÎµÎ¹ Ï„Î·Î½ AI ÏƒÏ„Î·Î½ ÎºÎ±Î¸Î·Î¼ÎµÏÎ¹Î½ÏŒÏ„Î·Ï„Î±.
        """)
    
    st.markdown('---')
    section_title('1.4 Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·: Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÎºÎ±Î¹ Î•Î¾ÎµÎ»Î¯Î¾ÎµÎ¹Ï‚')
    
    app_col1, app_col2 = st.columns(2)
    with app_col1:
        st.markdown("""
        #### ğŸ¥ Î¥Î³ÎµÎ¯Î±
        - Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· Î±ÏƒÎ¸ÎµÎ½ÎµÎ¹ÏÎ½ Î±Ï€ÏŒ Î¹Î±Ï„ÏÎ¹ÎºÎ­Ï‚ ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚
        - Î‘Î½Î±ÎºÎ¬Î»Ï…ÏˆÎ· Î½Î­Ï‰Î½ Ï†Î±ÏÎ¼Î¬ÎºÏ‰Î½
        - Î ÏÎ¿ÏƒÏ‰Ï€Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· Î¹Î±Ï„ÏÎ¹ÎºÎ®
        - Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÎµÏ€Î¹Î´Î·Î¼Î¹ÏÎ½
        
        #### ğŸš— ÎœÎµÏ„Î±Ï†Î¿ÏÎ­Ï‚
        - Î‘Ï…Ï„ÏŒÎ½Î¿Î¼Î± Î¿Ï‡Î®Î¼Î±Ï„Î±
        - Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î´Î¹Î±Î´ÏÎ¿Î¼ÏÎ½
        - ÎˆÎ¾Ï…Ï€Î½Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÎºÏ…ÎºÎ»Î¿Ï†Î¿ÏÎ¯Î±Ï‚
        - Î ÏÎ¿Î²Î»ÎµÏ€Ï„Î¹ÎºÎ® ÏƒÏ…Î½Ï„Î®ÏÎ·ÏƒÎ·
        """)
    with app_col2:
        st.markdown("""
        #### ğŸ’° Î§ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬
        - Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î±Ï€Î¬Ï„Î·Ï‚
        - Î‘Î»Î³Î¿ÏÎ¹Î¸Î¼Î¹ÎºÎ® Î´Î¹Î±Ï€ÏÎ±Î³Î¼Î¬Ï„ÎµÏ…ÏƒÎ·
        - Î Î¹ÏƒÏ„Î¿Î´Î¿Ï„Î¹ÎºÏŒÏ‚ Î­Î»ÎµÎ³Ï‡Î¿Ï‚
        - Robo-advisors
        
        #### ğŸ“ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·
        - Î ÏÎ¿ÏƒÏ‰Ï€Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· Î¼Î¬Î¸Î·ÏƒÎ·
        - Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î²Î±Î¸Î¼Î¿Î»ÏŒÎ³Î·ÏƒÎ·
        - Î•Î¹ÎºÎ¿Î½Î¹ÎºÎ¿Î¯ Î²Î¿Î·Î¸Î¿Î¯ Î´Î¹Î´Î±ÏƒÎºÎ±Î»Î¯Î±Ï‚
        - Î ÏÎ¿ÏƒÎ²Î±ÏƒÎ¹Î¼ÏŒÏ„Î·Ï„Î±
        """)
    
    st.markdown('---')
    section_title('1.5 Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÎˆÎ½Î½Î¿Î¹ÎµÏ‚ â€” Î Î»Î±Î¯ÏƒÎ¹Î¿ â€” ÎšÎ±Î½ÏŒÎ½ÎµÏ‚')
    
    subsection_title('1.5.1 Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÎˆÎ½Î½Î¿Î¹ÎµÏ‚')
    
    concepts_col1, concepts_col2, concepts_col3 = st.columns(3)
    with concepts_col1:
        st.markdown("""
        **Î•Ï€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· ÎœÎ¬Î¸Î·ÏƒÎ·**
        (Supervised Learning)
        - Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Îµ labeled data
        - Î ÏÏŒÎ²Î»ÎµÏˆÎ· outcomes
        - Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±: Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·, Regression
        """)
    with concepts_col2:
        st.markdown("""
        **ÎœÎ· Î•Ï€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· ÎœÎ¬Î¸Î·ÏƒÎ·**
        (Unsupervised Learning)
        - Î‘Î½Î±ÎºÎ¬Î»Ï…ÏˆÎ· patterns
        - Clustering
        - Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±: K-Means, PCA
        """)
    with concepts_col3:
        st.markdown("""
        **Î•Î½Î¹ÏƒÏ‡Ï…Ï„Î¹ÎºÎ® ÎœÎ¬Î¸Î·ÏƒÎ·**
        (Reinforcement Learning)
        - ÎœÎ¬Î¸Î·ÏƒÎ· Î¼Î­ÏƒÏ‰ Î´Î¿ÎºÎ¹Î¼Î®Ï‚-Î»Î¬Î¸Î¿Ï…Ï‚
        - Rewards/Penalties
        - Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±: Gaming AI, Î¡Î¿Î¼Ï€Î¿Ï„Î¹ÎºÎ®
        """)
    
    subsection_title('1.5.2 Î Î»Î±Î¯ÏƒÎ¹Î¿ Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚')
    st.markdown("""
    Î— AI ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÏ„Î±Î¹ ÏƒÎµ Î´Î¹Î¬Ï†Î¿ÏÎ± Ï€Î»Î±Î¯ÏƒÎ¹Î±:
    - **Î•ÏÎµÏ…Î½Î·Ï„Î¹ÎºÏŒ**: Î‘Î½Î¬Ï€Ï„Ï…Î¾Î· Î½Î­Ï‰Î½ Î±Î»Î³Î¿ÏÎ¯Î¸Î¼Ï‰Î½ ÎºÎ±Î¹ Ï„ÎµÏ‡Î½Î¹ÎºÏÎ½
    - **Î’Î¹Î¿Î¼Î·Ï‡Î±Î½Î¹ÎºÏŒ**: Î Î±ÏÎ±Î³Ï‰Î³Î¹ÎºÎ­Ï‚ ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚ ÏƒÎµ ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚
    - **ÎšÎ¿Î¹Î½Ï‰Î½Î¹ÎºÏŒ**: Î•Ï€Î¯Î»Ï…ÏƒÎ· ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÎºÏÎ½ Ï€ÏÎ¿Î²Î»Î·Î¼Î¬Ï„Ï‰Î½
    - **Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒ**: ÎœÎ¬Î¸Î·ÏƒÎ· ÎºÎ±Î¹ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·
    """)
    
    subsection_title('1.5.3 ÎšÎ±Î½ÏŒÎ½ÎµÏ‚ ÎºÎ±Î¹ Î—Î¸Î¹ÎºÎ®')
    st.warning("""
    âš ï¸ **Î—Î¸Î¹ÎºÎ­Ï‚ Î‘ÏÏ‡Î­Ï‚ ÏƒÏ„Î·Î½ AI**:
    - Î”Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± (Transparency)
    - Î”Î¹ÎºÎ±Î¹Î¿ÏƒÏÎ½Î· (Fairness)  
    - Î™Î´Î¹Ï‰Ï„Î¹ÎºÏŒÏ„Î·Ï„Î± (Privacy)
    - Î‘ÏƒÏ†Î¬Î»ÎµÎ¹Î± (Safety)
    - Î›Î¿Î³Î¿Î´Î¿ÏƒÎ¯Î± (Accountability)
    """)
    
    st.markdown('---')
    section_title('1.6 Î ÏÏ‚ Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ Ï„Î¿ ChatGPT')
    
    st.markdown("""
    ### ğŸ¤– Î¤Î¿ ChatGPT ÎµÎ¯Î½Î±Î¹ Î­Î½Î± Large Language Model (LLM)
    
    **Î’Î±ÏƒÎ¹ÎºÎ¬ Î§Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬:**
    """)
    
    chatgpt_col1, chatgpt_col2 = st.columns(2)
    with chatgpt_col1:
        st.markdown("""
        **Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ®:**
        - ğŸ”„ Î’Î±ÏƒÎ¯Î¶ÎµÏ„Î±Î¹ ÏƒÏ„Î·Î½ Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® **Transformer**
        - ğŸ“Š Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î¿ ÏƒÎµ Ï„ÎµÏÎ¬ÏƒÏ„Î¹Î¿ ÏŒÎ³ÎºÎ¿ ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½
        - ğŸ§® Î”Î¹ÏƒÎµÎºÎ±Ï„Î¿Î¼Î¼ÏÏÎ¹Î± Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹
        - ğŸ¯ Fine-tuned Î¼Îµ RLHF (Reinforcement Learning from Human Feedback)
        
        **Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±:**
        1. Î›Î±Î¼Î²Î¬Î½ÎµÎ¹ Ï„Î¿ input (prompt)
        2. Î¤Î¿ Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€ÎµÎ¹ ÏƒÎµ tokens
        3. Î•Ï€ÎµÎ¾ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹ Î¼Î­ÏƒÏ‰ transformer layers
        4. Î ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ Ï„Î¿ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ token
        5. Î Î±ÏÎ¬Î³ÎµÎ¹ ÏƒÏ…Î½ÎµÏ‡Î® ÎºÎµÎ¯Î¼ÎµÎ½Î¿
        """)
    with chatgpt_col2:
        st.markdown("""
        **Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚:**
        - âœï¸ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
        - ğŸ’¬ Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±
        - ğŸ“ Î£ÏÎ½Î¿ÏˆÎ·
        - ğŸ”„ ÎœÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ·
        - ğŸ’» Î ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚
        - ğŸ¨ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÏŒÏ„Î·Ï„Î±
        - ğŸ“Š Î‘Î½Î¬Î»Ï…ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        
        **Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯:**
        - âš ï¸ ÎœÏ€Î¿ÏÎµÎ¯ Î½Î± Ï€Î±ÏÎ¬Î³ÎµÎ¹ Î»Î¬Î¸Î·
        - ğŸ“… Cutoff date Î³Î½ÏÏƒÎ·Ï‚
        - ğŸ” Î”ÎµÎ½ Î±Î½Î±Î¶Î·Ï„Î¬ ÏƒÏ„Î¿ internet (GPT-3.5/4 base)
        - ğŸ¤” Î”ÎµÎ½ ÎºÎ±Ï„Î±Î½Î¿ÎµÎ¯ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬
        """)
    
    with st.expander('ğŸ”¬ Î¤ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ Î›ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚'):
        st.markdown("""
        **Pre-training:**
        - Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÏƒÎµ Ï„ÎµÏÎ¬ÏƒÏ„Î¹Î± datasets
        - Unsupervised learning
        - Next token prediction
        
        **Fine-tuning:**
        - Supervised fine-tuning (SFT)
        - Reinforcement Learning from Human Feedback (RLHF)
        - Alignment Î¼Îµ Î±Î½Î¸ÏÏÏ€Î¹Î½ÎµÏ‚ Ï€ÏÎ¿Ï„Î¹Î¼Î®ÏƒÎµÎ¹Ï‚
        
        **Transformer Architecture:**
        ```
        Input â†’ Tokenization â†’ Embeddings â†’ 
        Multi-Head Attention â†’ Feed Forward â†’ 
        Output Layer â†’ Generated Text
        ```
        """)
    
    st.markdown('---')
    section_title('1.7 Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ Î‘ÏÏ‡Î­Ï‚ AI ÎºÎ±Î¹ Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚')
    
    subsection_title('1.7.1 Machine Learning ÎºÎ±Î¹ Deep Learning')
    
    ml_col1, ml_col2 = st.columns(2)
    with ml_col1:
        st.markdown("""
        **Machine Learning (ML)**
        
        Î•Î¯Î½Î±Î¹ Î· Î¹ÎºÎ±Î½ÏŒÏ„Î·Ï„Î± Ï„Ï‰Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„ÏÎ½ Î½Î± Î¼Î±Î¸Î±Î¯Î½Î¿Ï…Î½ Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¯Î¶Î¿Î½Ï„Î±Î¹ ÏÎ·Ï„Î¬.
        
        **Î¤ÏÏ€Î¿Î¹:**
        1. **Supervised Learning**
           - Classification
           - Regression
           
        2. **Unsupervised Learning**
           - Clustering
           - Dimensionality Reduction
           
        3. **Reinforcement Learning**
           - Agent-based learning
           - Rewards optimization
        """)
    with ml_col2:
        st.markdown("""
        **Deep Learning (DL)**
        
        Î¥Ï€Î¿ÏƒÏÎ½Î¿Î»Î¿ Ï„Î¿Ï… ML Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î½ÎµÏ…ÏÏ‰Î½Î¹ÎºÎ¬ Î´Î¯ÎºÏ„Ï…Î± Î¼Îµ Ï€Î¿Î»Î»Î¬ ÎµÏ€Î¯Ï€ÎµÎ´Î± (layers).
        
        **Î‘ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ­Ï‚:**
        - **CNN** (Convolutional Neural Networks): Computer Vision
        - **RNN** (Recurrent Neural Networks): Sequences
        - **LSTM**: Long-term dependencies
        - **Transformers**: NLP, GPT, BERT
        - **GANs**: Generative models
        """)
    
    subsection_title('1.7.2 Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÎ® Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î· (Generative AI)')
    
    st.markdown("""
    Î— **Generative AI** Î±Î½Î±Ï†Î­ÏÎµÏ„Î±Î¹ ÏƒÎµ Î¼Î¿Î½Ï„Î­Î»Î± Ï€Î¿Ï… Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î½ Î½Î­Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿.
    
    **Î¤ÏÏ€Î¿Î¹ Generative AI:**
    """)
    
    gen_col1, gen_col2, gen_col3 = st.columns(3)
    with gen_col1:
        st.markdown("""
        **Text Generation**
        - GPT-4, ChatGPT
        - Claude
        - Gemini
        - LLaMA
        """)
    with gen_col2:
        st.markdown("""
        **Image Generation**
        - DALL-E
        - Midjourney
        - Stable Diffusion
        - Imagen
        """)
    with gen_col3:
        st.markdown("""
        **Other Modalities**
        - Music (MuseNet)
        - Video (Sora)
        - Voice (ElevenLabs)
        - Code (Copilot)
        """)
    
    subsection_title('1.7.3-1.7.7 Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ AI ÏƒÎµ Î”Î¹Î¬Ï†Î¿ÏÎ¿Ï…Ï‚ Î¤Î¿Î¼ÎµÎ¯Ï‚')
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs(['ğŸª Î Ï‰Î»Î®ÏƒÎµÎ¹Ï‚ & Marketing', 'ğŸ“ Î“ÏÎ±Î¼Î¼Î±Ï„ÎµÎ¯Î±', 'ğŸ’¼ Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚', 'ğŸ’µ Î§ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬', 'âš•ï¸ Î¥Î³ÎµÎ¯Î±'])
    
    with tab1:
        st.markdown("""
        **AI ÏƒÏ„Î¹Ï‚ Î Ï‰Î»Î®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Marketing**
        
        - **Personalization**: Î•Î¾Î±Ï„Î¿Î¼Î¹ÎºÎµÏ…Î¼Î­Î½ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Ï€ÏÎ¿ÏŠÏŒÎ½Ï„Ï‰Î½
        - **Chatbots**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÎ¾Ï…Ï€Î·ÏÎ­Ï„Î·ÏƒÎ· Ï€ÎµÎ»Î±Ï„ÏÎ½ 24/7
        - **Predictive Analytics**: Î ÏÏŒÎ²Î»ÎµÏˆÎ· ÏƒÏ…Î¼Ï€ÎµÏÎ¹Ï†Î¿ÏÎ¬Ï‚ Ï€ÎµÎ»Î±Ï„ÏÎ½
        - **Content Generation**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î´Î¹Î±Ï†Î·Î¼Î¹ÏƒÏ„Î¹ÎºÎ¿Ï Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï…
        - **Email Marketing**: Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· campaigns
        - **Social Media**: Î‘Î½Î¬Î»Ï…ÏƒÎ· sentiment, targeting
        """)
    
    with tab2:
        st.markdown("""
        **AI ÏƒÏ„Î· Î“ÏÎ±Î¼Î¼Î±Ï„ÎµÎ¯Î± ÎºÎ±Î¹ Î”Î¹Î¿Î¹ÎºÎ·Ï„Î¹ÎºÎ¬ Î£Ï„ÎµÎ»Î­Ï‡Î·**
        
        - **ÎˆÎ¾Ï…Ï€Î½Î· Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î•Î³Î³ÏÎ¬Ï†Ï‰Î½**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÎ±Î¹ Î±ÏÏ‡ÎµÎ¹Î¿Î¸Î­Ï„Î·ÏƒÎ·
        - **Scheduling**: ÎˆÎ¾Ï…Ï€Î½Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î·Î¼ÎµÏÎ¿Î»Î¿Î³Î¯Î¿Ï…
        - **Transcription**: ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î¿Î¼Î¹Î»Î¯Î±Ï‚ ÏƒÎµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
        - **Translation**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î¼ÎµÏ„Î¬Ï†ÏÎ±ÏƒÎ· ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½
        - **Data Entry**: Î‘Ï…Ï„Î¿Î¼Î±Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î®Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        """)
    
    with tab3:
        st.markdown("""
        **AI ÏƒÏ„Î· Î¨Î·Ï†Î¹Î±ÎºÎ® ÎœÎµÏ„Î±ÏƒÏ‡Î·Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒ Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½**
        
        - **Process Automation**: RPA (Robotic Process Automation)
        - **Decision Support**: Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Î»Î®ÏˆÎ·Ï‚ Î±Ï€Î¿Ï†Î¬ÏƒÎµÏ‰Î½
        - **Supply Chain**: Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎµÏ†Î¿Î´Î¹Î±ÏƒÏ„Î¹ÎºÎ®Ï‚ Î±Î»Ï…ÏƒÎ¯Î´Î±Ï‚
        - **Quality Control**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î¿Ï‚ Î­Î»ÎµÎ³Ï‡Î¿Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚
        - **Predictive Maintenance**: Î ÏÎ¿Î²Î»ÎµÏ€Ï„Î¹ÎºÎ® ÏƒÏ…Î½Ï„Î®ÏÎ·ÏƒÎ· ÎµÎ¾Î¿Ï€Î»Î¹ÏƒÎ¼Î¿Ï
        """)
    
    with tab4:
        st.markdown("""
        **AI ÏƒÏ„Î± Î§ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬**
        
        - **Fraud Detection**: Î‘Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î±Ï€Î¬Ï„Î·Ï‚ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿
        - **Algorithmic Trading**: Î‘Ï…Ï„Î¿Î¼Î±Ï„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚
        - **Credit Scoring**: Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Ï€Î¹ÏƒÏ„Î¿Î»Î·Ï€Ï„Î¹ÎºÎ®Ï‚ Î¹ÎºÎ±Î½ÏŒÏ„Î·Ï„Î±Ï‚
        - **Risk Management**: Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· ÎºÎ¹Î½Î´ÏÎ½Î¿Ï…
        - **Robo-Advisors**: Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· ÎµÏ€ÎµÎ½Î´Ï…Ï„Î¹ÎºÎ® ÏƒÏ…Î¼Î²Î¿Ï…Î»ÎµÏ…Ï„Î¹ÎºÎ®
        """)
    
    with tab5:
        st.markdown("""
        **AI ÏƒÏ„Î·Î½ Î¥Î³ÎµÎ¯Î±**
        
        - **Medical Imaging**: Î”Î¹Î¬Î³Î½Ï‰ÏƒÎ· Î±Ï€ÏŒ Î±ÎºÏ„Î¹Î½Î¿Î³ÏÎ±Ï†Î¯ÎµÏ‚, CT, MRI
        - **Drug Discovery**: Î‘Î½Î±ÎºÎ¬Î»Ï…ÏˆÎ· Î½Î­Ï‰Î½ Ï†Î±ÏÎ¼Î¬ÎºÏ‰Î½
        - **Personalized Medicine**: Î•Î¾Î±Ï„Î¿Î¼Î¹ÎºÎµÏ…Î¼Î­Î½Î· Î¸ÎµÏÎ±Ï€ÎµÎ¯Î±
        - **Patient Monitoring**: Î Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎ· Î±ÏƒÎ¸ÎµÎ½ÏÎ½
        - **Clinical Decision Support**: Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Î¹Î±Ï„ÏÎ¹ÎºÏÎ½ Î±Ï€Î¿Ï†Î¬ÏƒÎµÏ‰Î½
        """)
    
    st.markdown('---')
    st.success("""
    ğŸ“ **Î£Ï…Î³Ï‡Î±ÏÎ·Ï„Î®ÏÎ¹Î±!** ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÏƒÎ±Ï„Îµ Ï„Î¿ Î¸ÎµÏ‰ÏÎ·Ï„Î¹ÎºÏŒ Î¼Î­ÏÎ¿Ï‚. Î£Ï…Î½ÎµÏ‡Î¯ÏƒÏ„Îµ Î¼Îµ Ï„Î± Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ¬ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±!
    """)

with tabs[1]:
    section_title('Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± 1: Logistic Regression (Î•Ï€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· ÎœÎ¬Î¸Î·ÏƒÎ·)')
    
    st.markdown("""
    Î˜Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î¿ **Logistic Regression** Î³Î¹Î± **binary classification**.
    Î‘Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Î­Î½Î± ÎºÎ»Î±ÏƒÎ¹ÎºÏŒ Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎµÏ€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î·Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚.
    """)
    
    col1, col2 = st.columns([1, 1])
    with col1:
        n = st.slider('ğŸ”¢ Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½', 100, 2000, 500, step=100)
        noise = st.slider('ğŸ”Š Î•Ï€Î¯Ï€ÎµÎ´Î¿ Î¸Î¿ÏÏÎ²Î¿Ï…', 0.0, 0.5, 0.1, step=0.05)
    with col2:
        test_size = st.slider('ğŸ“Š Î Î¿ÏƒÎ¿ÏƒÏ„ÏŒ Test Set', 0.1, 0.5, 0.3, step=0.05)
        random_state = st.slider('ğŸ² Random Seed', 1, 100, 42)
    
    X, y = make_classification(n_samples=n, n_features=2, n_redundant=0, 
                                n_clusters_per_class=1, flip_y=noise, random_state=random_state)
    df = pd.DataFrame(X, columns=['Feature 1','Feature 2'])
    df['Target'] = y
    
    tab_viz1, tab_viz2, tab_viz3 = st.tabs(['ğŸ“Š Î”ÎµÎ´Î¿Î¼Î­Î½Î±', 'ğŸ“ˆ ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·', 'ğŸ¯ ÎœÎ¿Î½Ï„Î­Î»Î¿'])
    
    with tab_viz1:
        st.dataframe(df.head(20), use_container_width=True)
        st.markdown(f"**Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬:** {n} Î´ÎµÎ¯Î³Î¼Î±Ï„Î±, {df['Target'].value_counts().to_dict()}")
    
    with tab_viz2:
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(X[:,0], X[:,1], c=y, cmap='viridis', alpha=0.6, edgecolors='k')
        ax.set_xlabel('Feature 1', fontsize=12)
        ax.set_ylabel('Feature 2', fontsize=12)
        ax.set_title('Scatter Plot Ï„Ï‰Î½ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½', fontsize=14, fontweight='bold')
        plt.colorbar(scatter, ax=ax, label='Class')
        st.pyplot(fig)
        plt.close()
    
    with tab_viz3:
        if st.button('ğŸš€ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Logistic Regression'):
            with st.spinner('Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· Î¼Î¿Î½Ï„Î­Î»Î¿Ï…...'):
                time.sleep(1)  # Simulation
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
                model = LogisticRegression(max_iter=1000).fit(X_train, y_train)
                y_pred = model.predict(X_test)
                acc = accuracy_score(y_test, y_pred)
                
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric('âœ… Accuracy', f'{acc:.2%}')
                    st.metric('ğŸ“¦ Training Samples', len(X_train))
                with col_b:
                    st.metric('ğŸ“Š Test Samples', len(X_test))
                    st.metric('ğŸ¯ Correct Predictions', int(acc * len(X_test)))
                
                # Confusion Matrix
                cm = confusion_matrix(y_test, y_pred)
                fig_cm, ax_cm = plt.subplots(figsize=(8, 6))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm)
                ax_cm.set_title('Confusion Matrix')
                ax_cm.set_xlabel('Predicted')
                ax_cm.set_ylabel('Actual')
                st.pyplot(fig_cm)
                plt.close()
                
                # Visualization
                fig2, ax2 = plt.subplots(figsize=(10, 6))
                scatter = ax2.scatter(X_test[:,0], X_test[:,1], c=y_pred, cmap='coolwarm', 
                                     alpha=0.7, edgecolors='k', s=100)
                ax2.set_xlabel('Feature 1')
                ax2.set_ylabel('Feature 2')
                ax2.set_title('Î ÏÎ¿Î²Î»Î­ÏˆÎµÎ¹Ï‚ ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…')
                plt.colorbar(scatter, ax=ax2)
                st.pyplot(fig2)
                plt.close()
                
                st.success(f'âœ… Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎºÏ€Î±Î¹Î´ÎµÏÏ„Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚ Î¼Îµ accuracy {acc:.2%}!')

    st.markdown('---')
    section_title('Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± 2: K-Means Clustering (ÎœÎ· Î•Ï€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· ÎœÎ¬Î¸Î·ÏƒÎ·)')
    
    st.markdown("""
    Î¤Î¿ **K-Means** ÎµÎ¯Î½Î±Î¹ Î­Î½Î±Ï‚ Î´Î·Î¼Î¿Ï†Î¹Î»Î®Ï‚ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚ **clustering** (Î¿Î¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚) Ï€Î¿Ï… Î±Î½Î®ÎºÎµÎ¹ ÏƒÏ„Î· 
    Î¼Î· ÎµÏ€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î¼Î¬Î¸Î·ÏƒÎ·.
    """)
    
    col_km1, col_km2 = st.columns(2)
    with col_km1:
        n_samples_km = st.slider('ğŸ“¦ Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½', 100, 1000, 400, key='km_samples')
        n_clusters = st.slider('ğŸ¯ Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Clusters (K)', 2, 8, 3, key='kc')
    with col_km2:
        cluster_std = st.slider('ğŸ“ Cluster Standard Deviation', 0.5, 3.0, 1.5, key='km_std')
        random_km = st.slider('ğŸ² Random State', 1, 100, 42, key='km_random')
    
    if st.button('ğŸ”¬ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· K-Means'):
        with st.spinner('Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Î±Î»Î³Î¿ÏÎ¯Î¸Î¼Î¿Ï…...'):
            from sklearn.cluster import KMeans
            Xc, yc = make_blobs(n_samples=n_samples_km, centers=n_clusters, 
                               n_features=2, cluster_std=cluster_std, random_state=random_km)
            km = KMeans(n_clusters=n_clusters, random_state=random_km, n_init=10).fit(Xc)
            labels = km.labels_
            centers = km.cluster_centers_
            
            fig_km, ax_km = plt.subplots(figsize=(10, 6))
            scatter = ax_km.scatter(Xc[:,0], Xc[:,1], c=labels, cmap='tab10', alpha=0.6, edgecolors='k')
            ax_km.scatter(centers[:,0], centers[:,1], c='red', marker='X', s=300, 
                         edgecolors='black', linewidths=2, label='Centroids')
            ax_km.set_xlabel('Feature 1')
            ax_km.set_ylabel('Feature 2')
            ax_km.set_title(f'K-Means Clustering (K={n_clusters})')
            ax_km.legend()
            st.pyplot(fig_km)
            plt.close()
            
            st.success(f'âœ… Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¿Î¼Î±Î´Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎ±Î½ ÏƒÎµ {n_clusters} clusters!')
            
            # Show cluster sizes
            unique, counts = np.unique(labels, return_counts=True)
            cluster_info = pd.DataFrame({'Cluster': unique, 'Size': counts})
            st.dataframe(cluster_info, use_container_width=True, hide_index=True)
    
    st.markdown('---')
    section_title('Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± 3: ÎÎµÏ…ÏÏ‰Î½Î¹ÎºÏŒ Î”Î¯ÎºÏ„Ï…Î¿ (Deep Learning)')
    
    st.markdown("""
    ÎˆÎ½Î± Î±Ï€Î»ÏŒ **Neural Network** Î³Î¹Î± classification. Î˜Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ scikit-learn 
    Î³Î¹Î± Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î­Î½Î± multi-layer perceptron.
    """)
    
    col_nn1, col_nn2 = st.columns(2)
    with col_nn1:
        hidden_layers = st.multiselect('ğŸ§  Hidden Layers (neurons)', 
                                       [10, 20, 50, 100, 200], default=[100, 50])
        activation = st.selectbox('âš¡ Activation Function', ['relu', 'tanh', 'logistic'])
    with col_nn2:
        learning_rate = st.select_slider('ğŸ“ˆ Learning Rate', 
                                         options=[0.0001, 0.001, 0.01, 0.1], value=0.001)
        max_iterations = st.slider('ğŸ”„ Max Iterations', 100, 1000, 200, step=100)
    
    if st.button('ğŸš€ Train Neural Network'):
        from sklearn.neural_network import MLPClassifier
        from sklearn.preprocessing import StandardScaler
        
        with st.spinner('Training Neural Network...'):
            X_nn, y_nn = make_classification(n_samples=1000, n_features=20, n_informative=15,
                                            n_redundant=5, random_state=42)
            X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(
                X_nn, y_nn, test_size=0.3, random_state=42)
            
            scaler = StandardScaler()
            X_train_nn = scaler.fit_transform(X_train_nn)
            X_test_nn = scaler.transform(X_test_nn)
            
            if hidden_layers:
                mlp = MLPClassifier(hidden_layer_sizes=tuple(hidden_layers),
                                   activation=activation, 
                                   learning_rate_init=learning_rate,
                                   max_iter=max_iterations,
                                   random_state=42)
                mlp.fit(X_train_nn, y_train_nn)
                y_pred_nn = mlp.predict(X_test_nn)
                acc_nn = accuracy_score(y_test_nn, y_pred_nn)
                
                col_nn_a, col_nn_b, col_nn_c = st.columns(3)
                with col_nn_a:
                    st.metric('ğŸ¯ Test Accuracy', f'{acc_nn:.2%}')
                with col_nn_b:
                    st.metric('ğŸ“Š Iterations', mlp.n_iter_)
                with col_nn_c:
                    st.metric('ğŸ“‰ Final Loss', f'{mlp.loss_:.4f}')
                
                # Plot loss curve
                fig_loss, ax_loss = plt.subplots(figsize=(10, 5))
                ax_loss.plot(mlp.loss_curve_, linewidth=2)
                ax_loss.set_xlabel('Iterations')
                ax_loss.set_ylabel('Loss')
                ax_loss.set_title('Training Loss Curve')
                ax_loss.grid(True, alpha=0.3)
                st.pyplot(fig_loss)
                plt.close()
                
                st.success(f'âœ… Neural Network trained with accuracy: {acc_nn:.2%}!')
            else:
                st.error('Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÏ€Î¹Î»Î­Î¾Ï„Îµ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ Î­Î½Î± hidden layer!')

with tabs[2]:
    section_title('Î•Î¾Î¿Î¼Î¿Î¹ÏÏƒÎµÎ¹Ï‚ AI â€” Î”Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ¬ Demos')
    
    st.markdown("""
    Î£Îµ Î±Ï…Ï„Î® Ï„Î·Î½ ÎµÎ½ÏŒÏ„Î·Ï„Î± Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Ï€ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÏ„ÎµÎ¯Ï„Îµ Î¼Îµ Î´Î¹Î¬Ï†Î¿ÏÎµÏ‚ Ï€Î±ÏÎ±Î¼Î­Ï„ÏÎ¿Ï…Ï‚ ÎºÎ±Î¹ Î½Î± Î´ÎµÎ¯Ï„Îµ Ï€ÏÏ‚ 
    ÎµÏ€Î·ÏÎµÎ¬Î¶Î¿Ï…Î½ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Ï„Ï‰Î½ Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ AI.
    """)
    
    sim_option = st.selectbox('ğŸ® Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î•Î¾Î¿Î¼Î¿Î¯Ï‰ÏƒÎ·:', [
        'Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· Î˜Î¿ÏÏÎ²Î¿Ï… ÏƒÏ„Î·Î½ Î‘ÎºÏÎ¯Î²ÎµÎ¹Î±',
        'Overfitting vs Underfitting',
        'Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· ÎœÎµÎ³Î­Î¸Î¿Ï…Ï‚ Dataset',
        'Decision Boundary Visualization'
    ])
    
    if sim_option == 'Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· Î˜Î¿ÏÏÎ²Î¿Ï… ÏƒÏ„Î·Î½ Î‘ÎºÏÎ¯Î²ÎµÎ¹Î±':
        st.markdown('### ğŸ”Š Î ÏÏ‚ Î¿ Î¸ÏŒÏÏ…Î²Î¿Ï‚ ÎµÏ€Î·ÏÎµÎ¬Î¶ÎµÎ¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿')
        
        noise_level = st.slider('Î•Ï€Î¯Ï€ÎµÎ´Î¿ Î¸Î¿ÏÏÎ²Î¿Ï… (flip_y)', 0.0, 0.5, 0.1, 0.05)
        
        results = []
        for noise in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]:
            X_sim, y_sim = make_classification(n_samples=500, n_features=2, n_redundant=0, 
                                               flip_y=noise, random_state=42)
            X_tr, X_te, y_tr, y_te = train_test_split(X_sim, y_sim, test_size=0.3, random_state=42)
            model_sim = LogisticRegression().fit(X_tr, y_tr)
            acc_sim = accuracy_score(y_te, model_sim.predict(X_te))
            results.append({'Noise': noise, 'Accuracy': acc_sim})
        
        df_results = pd.DataFrame(results)
        
        fig_noise, ax_noise = plt.subplots(figsize=(10, 6))
        ax_noise.plot(df_results['Noise'], df_results['Accuracy'], marker='o', linewidth=2, markersize=8)
        ax_noise.axvline(x=noise_level, color='r', linestyle='--', label=f'Current ({noise_level})')
        ax_noise.set_xlabel('Noise Level', fontsize=12)
        ax_noise.set_ylabel('Accuracy', fontsize=12)
        ax_noise.set_title('Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· Î˜Î¿ÏÏÎ²Î¿Ï… ÏƒÏ„Î·Î½ Î‘ÎºÏÎ¯Î²ÎµÎ¹Î± Ï„Î¿Ï… ÎœÎ¿Î½Ï„Î­Î»Î¿Ï…', fontsize=14, fontweight='bold')
        ax_noise.grid(True, alpha=0.3)
        ax_noise.legend()
        st.pyplot(fig_noise)
        plt.close()
        
        st.info(f'ğŸ“Š ÎœÎµ noise level {noise_level}, Î· Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÎµÎ¯Î½Î±Î¹ Ï€ÎµÏÎ¯Ï€Î¿Ï… {df_results[df_results["Noise"]==noise_level]["Accuracy"].values[0]:.2%}')
    
    elif sim_option == 'Overfitting vs Underfitting':
        st.markdown('### âš–ï¸ Overfitting vs Underfitting')
        
        st.markdown("""
        - **Underfitting**: Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î±Ï€Î»ÏŒ ÎºÎ±Î¹ Î´ÎµÎ½ Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹ ÎºÎ±Î»Î¬
        - **Good Fit**: Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î¿
        - **Overfitting**: Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î­Ï‡ÎµÎ¹ "Î¼Î¬Î¸ÎµÎ¹" Ï„Î¿Î½ training set Î±Ï€' Î­Î¾Ï‰ Î±Î»Î»Î¬ Î´ÎµÎ½ Î³ÎµÎ½Î¹ÎºÎµÏÎµÎ¹
        """)
        
        from sklearn.preprocessing import PolynomialFeatures
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import mean_squared_error
        
        # Generate data
        np.random.seed(42)
        X_poly = np.sort(np.random.rand(50, 1) * 10, axis=0)
        y_poly = np.sin(X_poly).ravel() + np.random.randn(50) * 0.5
        
        degree = st.slider('ğŸ¯ Polynomial Degree', 1, 15, 3)
        
        poly_features = PolynomialFeatures(degree=degree)
        X_poly_transformed = poly_features.fit_transform(X_poly)
        model_poly = LinearRegression().fit(X_poly_transformed, y_poly)
        
        X_test_poly = np.linspace(0, 10, 100).reshape(-1, 1)
        X_test_poly_transformed = poly_features.transform(X_test_poly)
        y_pred_poly = model_poly.predict(X_test_poly_transformed)
        
        fig_poly, ax_poly = plt.subplots(figsize=(10, 6))
        ax_poly.scatter(X_poly, y_poly, color='blue', s=50, alpha=0.6, label='Data')
        ax_poly.plot(X_test_poly, y_pred_poly, color='red', linewidth=2, label=f'Degree {degree}')
        ax_poly.set_xlabel('X')
        ax_poly.set_ylabel('y')
        ax_poly.set_title(f'Polynomial Regression (Degree={degree})')
        ax_poly.legend()
        ax_poly.grid(True, alpha=0.3)
        st.pyplot(fig_poly)
        plt.close()
        
        mse = mean_squared_error(y_poly, model_poly.predict(X_poly_transformed))
        st.metric('ğŸ“‰ Training MSE', f'{mse:.4f}')
        
        if degree < 3:
            st.warning('âš ï¸ **Underfitting**: Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î±Ï€Î»ÏŒ!')
        elif degree <= 5:
            st.success('âœ… **Good Fit**: Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ Î¹ÏƒÎ¿ÏÏÎ¿Ï€Î·Î¼Î­Î½Î¿!')
        else:
            st.error('ğŸ”´ **Overfitting**: Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Ï€Î¿Î»ÏÏ€Î»Î¿ÎºÎ¿!')
    
    elif sim_option == 'Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· ÎœÎµÎ³Î­Î¸Î¿Ï…Ï‚ Dataset':
        st.markdown('### ğŸ“Š Î ÏÏ‚ Ï„Î¿ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ Ï„Î¿Ï… dataset ÎµÏ€Î·ÏÎµÎ¬Î¶ÎµÎ¹ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ·')
        
        sizes = [50, 100, 200, 500, 1000, 2000]
        accuracies = []
        
        with st.spinner('Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï€ÎµÎ¹ÏÎ±Î¼Î¬Ï„Ï‰Î½...'):
            for size in sizes:
                X_size, y_size = make_classification(n_samples=size, n_features=10, random_state=42)
                X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(X_size, y_size, test_size=0.3, random_state=42)
                model_size = LogisticRegression(max_iter=1000).fit(X_tr_s, y_tr_s)
                acc_size = accuracy_score(y_te_s, model_size.predict(X_te_s))
                accuracies.append(acc_size)
        
        fig_size, ax_size = plt.subplots(figsize=(10, 6))
        ax_size.plot(sizes, accuracies, marker='o', linewidth=2, markersize=10, color='green')
        ax_size.set_xlabel('Dataset Size', fontsize=12)
        ax_size.set_ylabel('Accuracy', fontsize=12)
        ax_size.set_title('Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· ÎœÎµÎ³Î­Î¸Î¿Ï…Ï‚ Dataset ÏƒÏ„Î·Î½ Î‘ÎºÏÎ¯Î²ÎµÎ¹Î±', fontsize=14, fontweight='bold')
        ax_size.grid(True, alpha=0.3)
        st.pyplot(fig_size)
        plt.close()
        
        st.info('ğŸ’¡ **Î Î±ÏÎ±Ï„Î®ÏÎ·ÏƒÎ·**: ÎœÎµ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î´ÎµÎ´Î¿Î¼Î­Î½Î±, Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î³ÎµÎ½Î¹ÎºÎ¬ Î²ÎµÎ»Ï„Î¹ÏÎ½ÎµÏ„Î±Î¹!')
    
    elif sim_option == 'Decision Boundary Visualization':
        st.markdown('### ğŸ¯ ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Decision Boundary')
        
        from sklearn.svm import SVC
        
        kernel_choice = st.selectbox('Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Kernel:', ['linear', 'rbf', 'poly'])
        
        X_db, y_db = make_classification(n_samples=200, n_features=2, n_redundant=0, 
                                         n_clusters_per_class=1, random_state=42)
        
        model_db = SVC(kernel=kernel_choice, gamma='auto').fit(X_db, y_db)
        
        # Create mesh
        h = 0.02
        x_min, x_max = X_db[:, 0].min() - 1, X_db[:, 0].max() + 1
        y_min, y_max = X_db[:, 1].min() - 1, X_db[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        
        Z = model_db.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        fig_db, ax_db = plt.subplots(figsize=(10, 8))
        ax_db.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
        scatter = ax_db.scatter(X_db[:, 0], X_db[:, 1], c=y_db, cmap='coolwarm', 
                               edgecolors='k', s=50)
        ax_db.set_xlabel('Feature 1')
        ax_db.set_ylabel('Feature 2')
        ax_db.set_title(f'Decision Boundary (Kernel: {kernel_choice})')
        plt.colorbar(scatter, ax=ax_db)
        st.pyplot(fig_db)
        plt.close()
        
        acc_db = model_db.score(X_db, y_db)
        st.metric('ğŸ¯ Training Accuracy', f'{acc_db:.2%}')

with tabs[3]:
    section_title('ÎšÎ¿Ï…Î¯Î¶ Î‘Ï…Ï„Î¿Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚')
    
    st.markdown("""
    Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Ï„Î¹Ï‚ Î³Î½ÏÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚! Î‘Ï€Î±Î½Ï„Î®ÏƒÏ„Îµ ÏƒÏ„Î¹Ï‚ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î½Î± ÎµÎ»Î­Î³Î¾ÎµÏ„Îµ Ï„Î·Î½ ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ® ÏƒÎ±Ï‚.
    """)
    
    quiz_category = st.selectbox('ğŸ“š Î•Ï€Î¹Î»Î­Î¾Ï„Îµ ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯Î±:', [
        'Î“ÎµÎ½Î¹ÎºÎ¬ Î³Î¹Î± AI',
        'Machine Learning',
        'ChatGPT & LLMs',
        'Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ AI',
        'Î—Î¸Î¹ÎºÎ® & ÎšÎ¿Î¹Î½Ï‰Î½Î¯Î±'
    ])
    
    quizzes_general = [
        {
            'id':'q1', 'type':'mcq',
            'question':'Î¤Î¹ ÎµÎ¯Î½Î±Î¹ Î· ÎœÎ·Ï‡Î±Î½Î¹ÎºÎ® ÎœÎ¬Î¸Î·ÏƒÎ· (Machine Learning);',
            'options':[
                'Î“ÏÎ±Ï†Î¹ÎºÎ¬ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„ÏÎ½',
                'Î¥Ï€Î¿ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Ï„Î·Ï‚ AI Ï€Î¿Ï… Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹ Î±Ï€ÏŒ Î´ÎµÎ´Î¿Î¼Î­Î½Î±',
                'ÎœÏŒÎ½Î¿ Î´Î¯ÎºÏ„Ï…Î± Î½ÎµÏ…ÏÏÎ½Ï‰Î½',
                'ÎˆÎ½Î± ÎµÎ¯Î´Î¿Ï‚ Î²Î¬ÏƒÎ·Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½'
            ],
            'answer':'Î¥Ï€Î¿ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Ï„Î·Ï‚ AI Ï€Î¿Ï… Î¼Î±Î¸Î±Î¯Î½ÎµÎ¹ Î±Ï€ÏŒ Î´ÎµÎ´Î¿Î¼Î­Î½Î±',
            'explain':'Î— ML ÎµÎ¯Î½Î±Î¹ Î· Ï…Ï€Î¿ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Ï„Î·Ï‚ AI Ï€Î¿Ï… ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ ÏƒÎµ Î±Î»Î³Î¿ÏÎ¯Î¸Î¼Î¿Ï…Ï‚ Î½Î± Î¼Î±Î¸Î±Î¯Î½Î¿Ï…Î½ Î±Ï€ÏŒ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¯Î¶Î¿Î½Ï„Î±Î¹ ÏÎ·Ï„Î¬.'
        },
        {
            'id':'q2', 'type':'tf',
            'question':'Î— Î¤ÎµÏ‡Î½Î·Ï„Î® ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ Ï„Î¿ 2020.',
            'answer':False,
            'explain':'Î›Î¬Î¸Î¿Ï‚! ÎŸ ÏŒÏÎ¿Ï‚ "Artificial Intelligence" Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎµ Î³Î¹Î± Ï€ÏÏÏ„Î· Ï†Î¿ÏÎ¬ Ï„Î¿ 1956 ÏƒÏ„Î¿ Dartmouth Conference.'
        },
        {
            'id':'q3', 'type':'mcq',
            'question':'Î Î¿Î¹Î¿ Î±Ï€ÏŒ Ï„Î± Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î”Î•Î ÎµÎ¯Î½Î±Î¹ Î²Î±ÏƒÎ¹ÎºÏŒ Î´Î¿Î¼Î¹ÎºÏŒ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î¿ Ï„Î·Ï‚ AI;',
            'options':[
                'Î”ÎµÎ´Î¿Î¼Î­Î½Î±',
                'Î‘Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Î¹',
                'ÎœÎ¿Î½Ï„Î­Î»Î±',
                'Î¤Ï…Ï€Î¿Î³ÏÎ±Ï†Î¯Î±'
            ],
            'answer':'Î¤Ï…Ï€Î¿Î³ÏÎ±Ï†Î¯Î±',
            'explain':'Î¤Î± Î²Î±ÏƒÎ¹ÎºÎ¬ Î´Î¿Î¼Î¹ÎºÎ¬ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± Ï„Î·Ï‚ AI ÎµÎ¯Î½Î±Î¹: Î”ÎµÎ´Î¿Î¼Î­Î½Î±, Î‘Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Î¹, ÎœÎ¿Î½Ï„Î­Î»Î± ÎºÎ±Î¹ Î¥Ï€Î¿Î´Î¿Î¼Î­Ï‚.'
        }
    ]
    
    quizzes_ml = [
        {
            'id':'q4', 'type':'mcq',
            'question':'Î Î¿Î¹Î¿Ï‚ Ï„ÏÏ€Î¿Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ labeled data;',
            'options':[
                'Unsupervised Learning',
                'Supervised Learning',
                'Reinforcement Learning',
                'Transfer Learning'
            ],
            'answer':'Supervised Learning',
            'explain':'Î— Supervised Learning Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ labeled data (Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Îµ ÎµÏ„Î¹ÎºÎ­Ï„ÎµÏ‚) Î³Î¹Î± Î½Î± ÎµÎºÏ€Î±Î¹Î´ÎµÏÏƒÎµÎ¹ Î¼Î¿Î½Ï„Î­Î»Î±.'
        },
        {
            'id':'q5', 'type':'tf',
            'question':'Î¤Î¿ K-Means ÎµÎ¯Î½Î±Î¹ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚ ÎµÏ€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î·Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚.',
            'answer':False,
            'explain':'Î›Î¬Î¸Î¿Ï‚! Î¤Î¿ K-Means ÎµÎ¯Î½Î±Î¹ Î±Î»Î³ÏŒÏÎ¹Î¸Î¼Î¿Ï‚ Î¼Î· ÎµÏ€Î¹Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î·Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚ (unsupervised) Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î³Î¹Î± clustering.'
        },
        {
            'id':'q6', 'type':'mcq',
            'question':'Î¤Î¹ ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ "Overfitting";',
            'options':[
                'Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î±Ï€Î»ÏŒ',
                'Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î­Ï‡ÎµÎ¹ Ï…ÏˆÎ·Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î¿ training set Î±Î»Î»Î¬ Ï‡Î±Î¼Î·Î»Î® ÏƒÏ„Î¿ test set',
                'Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ÎµÎºÏ€Î±Î¹Î´ÎµÏÎµÏ„Î±Î¹ Ï€Î¿Î»Ï Î³ÏÎ®Î³Î¿ÏÎ±',
                'Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î­Ï‡ÎµÎ¹ Ï€Î¿Î»Î»Î¬ features'
            ],
            'answer':'Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î­Ï‡ÎµÎ¹ Ï…ÏˆÎ·Î»Î® Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î¿ training set Î±Î»Î»Î¬ Ï‡Î±Î¼Î·Î»Î® ÏƒÏ„Î¿ test set',
            'explain':'Overfitting ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ "Î­Î¼Î±Î¸Îµ" Ï„Î¿Î½ training set Î±Ï€\' Î­Î¾Ï‰ Î±Î»Î»Î¬ Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î³ÎµÎ½Î¹ÎºÎµÏÏƒÎµÎ¹ ÏƒÎµ Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.'
        }
    ]
    
    quizzes_chatgpt = [
        {
            'id':'q7', 'type':'tf',
            'question':'Î¤Î¿ ChatGPT ÎµÎ¯Î½Î±Î¹ transformer-based language model.',
            'answer':True,
            'explain':'Î£Ï‰ÏƒÏ„Î¬! Î¤Î¿ ChatGPT Î²Î±ÏƒÎ¯Î¶ÎµÏ„Î±Î¹ ÏƒÏ„Î·Î½ Î±ÏÏ‡Î¹Ï„ÎµÎºÏ„Î¿Î½Î¹ÎºÎ® Transformer Ï€Î¿Ï… ÎµÎ¹ÏƒÎ®Ï‡Î¸Î· Ï„Î¿ 2017.'
        },
        {
            'id':'q8', 'type':'mcq',
            'question':'Î¤Î¹ ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ "LLM";',
            'options':[
                'Large Language Model',
                'Low Level Machine',
                'Linear Learning Method',
                'Logical Language Mechanism'
            ],
            'answer':'Large Language Model',
            'explain':'LLM ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ Large Language Model - Î¼ÎµÎ³Î¬Î»Î± Î³Î»Ï‰ÏƒÏƒÎ¹ÎºÎ¬ Î¼Î¿Î½Ï„Î­Î»Î± ÏŒÏ€Ï‰Ï‚ Ï„Î¿ GPT-4, Claude, ÎºÎ»Ï€.'
        },
        {
            'id':'q9', 'type':'mcq',
            'question':'Î¤Î¹ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ "RLHF" Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÏƒÏ„Î¿ ChatGPT;',
            'options':[
                'Random Learning from Humans',
                'Reinforcement Learning from Human Feedback',
                'Rapid Language Handling Function',
                'Real-time Learning for High Frequency'
            ],
            'answer':'Reinforcement Learning from Human Feedback',
            'explain':'RLHF = Reinforcement Learning from Human Feedback. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î³Î¹Î± Î½Î± "ÎµÏ…Î¸Ï…Î³ÏÎ±Î¼Î¼Î¯ÏƒÎµÎ¹" Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¼Îµ Î±Î½Î¸ÏÏÏ€Î¹Î½ÎµÏ‚ Ï€ÏÎ¿Ï„Î¹Î¼Î®ÏƒÎµÎ¹Ï‚.'
        }
    ]
    
    quizzes_applications = [
        {
            'id':'q10', 'type':'mcq',
            'question':'Î£Îµ Ï€Î¿Î¹Î¿Î½ Ï„Î¿Î¼Î­Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î· AI Î³Î¹Î± Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î±Ï€Î¬Ï„Î·Ï‚;',
            'options':[
                'Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·',
                'Î§ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬',
                'Î¨Ï…Ï‡Î±Î³Ï‰Î³Î¯Î±',
                'Î‘Î¸Î»Î·Ï„Î¹ÏƒÎ¼ÏŒÏ‚'
            ],
            'answer':'Î§ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬',
            'explain':'Î— AI Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÎµÏ…ÏÎ­Ï‰Ï‚ ÏƒÏ„Î± Ï‡ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬ Î³Î¹Î± Î±Î½Î¯Ï‡Î½ÎµÏ…ÏƒÎ· Î±Ï€Î¬Ï„Î·Ï‚ ÏƒÎµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ Ï‡ÏÏŒÎ½Î¿.'
        },
        {
            'id':'q11', 'type':'tf',
            'question':'Î— AI Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î´Î¹Î±Î³Î½ÏÏƒÎµÎ¹ Î±ÏƒÎ¸Î­Î½ÎµÎ¹ÎµÏ‚ Î±Ï€ÏŒ Î¹Î±Ï„ÏÎ¹ÎºÎ­Ï‚ ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚.',
            'answer':True,
            'explain':'Î£Ï‰ÏƒÏ„Î¬! Î— AI (ÎµÎ¹Î´Î¹ÎºÎ¬ Computer Vision) Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î³Î¹Î± Î½Î± Î±Î½Î±Î»ÏÏƒÎµÎ¹ Î±ÎºÏ„Î¹Î½Î¿Î³ÏÎ±Ï†Î¯ÎµÏ‚, CT, MRI ÎºÎ»Ï€.'
        },
        {
            'id':'q12', 'type':'mcq',
            'question':'Î Î¿Î¹Î± Î±Ï€ÏŒ Ï„Î¹Ï‚ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î”Î•Î ÎµÎ¯Î½Î±Î¹ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Generative AI;',
            'options':[
                'DALL-E (Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎµÎ¹ÎºÏŒÎ½Ï‰Î½)',
                'ChatGPT (Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…)',
                'Spam Filter (Ï†Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± email)',
                'Midjourney (Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎµÎ¹ÎºÏŒÎ½Ï‰Î½)'
            ],
            'answer':'Spam Filter (Ï†Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± email)',
            'explain':'Î¤Î¿ spam filter ÎµÎ¯Î½Î±Î¹ classification task, ÏŒÏ‡Î¹ generative. Î¤Î± Î¬Î»Î»Î± Ï„ÏÎ¯Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ½ Î½Î­Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿.'
        }
    ]
    
    quizzes_ethics = [
        {
            'id':'q13', 'type':'tf',
            'question':'Î— AI Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ Î¼ÎµÏÎ¿Î»Î·Ï€Ï„Î¹ÎºÎ® (biased) Î±Î½ ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„ÎµÎ¯ ÏƒÎµ Î¼ÎµÏÎ¿Î»Î·Ï€Ï„Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î±.',
            'answer':True,
            'explain':'Î£Ï‰ÏƒÏ„Î¬! Î¤Î¿ bias ÏƒÏ„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼ÎµÏ„Î±Ï†Î­ÏÎµÏ„Î±Î¹ ÏƒÏ„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿. Î“Î¹\' Î±Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÎ® Î· Î´Î¹ÎºÎ±Î¹Î¿ÏƒÏÎ½Î· ÎºÎ±Î¹ Î´Î¹Î±Ï†Î¬Î½ÎµÎ¹Î± ÏƒÏ„Î·Î½ AI.'
        },
        {
            'id':'q14', 'type':'mcq',
            'question':'Î Î¿Î¹Î± Î·Î¸Î¹ÎºÎ® Î±ÏÏ‡Î® Î±Ï†Î¿ÏÎ¬ Ï„Î¿ Î´Î¹ÎºÎ±Î¯Ï‰Î¼Î± Ï„Ï‰Î½ Î±Î½Î¸ÏÏÏ€Ï‰Î½ Î½Î± Î³Î½Ï‰ÏÎ¯Î¶Î¿Ï…Î½ Ï€ÏÏ‚ Ï€Î±Î¯ÏÎ½Î¿Î½Ï„Î±Î¹ Î±Ï€Î¿Ï†Î¬ÏƒÎµÎ¹Ï‚ Î±Ï€ÏŒ AI;',
            'options':[
                'Privacy',
                'Transparency',
                'Accountability',
                'Fairness'
            ],
            'answer':'Transparency',
            'explain':'Î— Transparency (Î”Î¹Î±Ï†Î¬Î½ÎµÎ¹Î±) ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Ï„Î± AI ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ ÎºÎ±Ï„Î±Î½Î¿Î·Ï„Î¬ ÎºÎ±Î¹ explainable.'
        },
        {
            'id':'q15', 'type':'mcq',
            'question':'Î Î¿Î¹Î¿ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ Î·Î¸Î¹ÎºÏŒ Ï€ÏÏŒÎ²Î»Î·Î¼Î± Î¼Îµ Ï„Î± Large Language Models;',
            'options':[
                'ÎšÎ±Ï„Î±Î½Î¬Î»Ï‰ÏƒÎ· ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±Ï‚',
                'Î Î±ÏÎ±Î³Ï‰Î³Î® misinformation ÎºÎ±Î¹ hallucinations',
                'ÎšÏŒÏƒÏ„Î¿Ï‚ training',
                'Î¤Î±Ï‡ÏÏ„Î·Ï„Î± Î±Ï€ÏŒÎºÏÎ¹ÏƒÎ·Ï‚'
            ],
            'answer':'Î Î±ÏÎ±Î³Ï‰Î³Î® misinformation ÎºÎ±Î¹ hallucinations',
            'explain':'Î¤Î± LLMs Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€Î±ÏÎ¬Î³Î¿Ï…Î½ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Ï€Î¿Ï… Î±ÎºÎ¿ÏÎ³Î¿Î½Ï„Î±Î¹ Ï€ÎµÎ¹ÏƒÏ„Î¹ÎºÎ­Ï‚ Î±Î»Î»Î¬ ÎµÎ¯Î½Î±Î¹ Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½ÎµÏ‚ (hallucinations).'
        }
    ]
    
    if quiz_category == 'Î“ÎµÎ½Î¹ÎºÎ¬ Î³Î¹Î± AI':
        quizzes_to_show = quizzes_general
    elif quiz_category == 'Machine Learning':
        quizzes_to_show = quizzes_ml
    elif quiz_category == 'ChatGPT & LLMs':
        quizzes_to_show = quizzes_chatgpt
    elif quiz_category == 'Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ AI':
        quizzes_to_show = quizzes_applications
    else:
        quizzes_to_show = quizzes_ethics
    
    for i, q in enumerate(quizzes_to_show, 1):
        with st.container():
            st.markdown(f"#### Î•ÏÏÏ„Î·ÏƒÎ· {i}")
            show_quiz(q)
            st.markdown('---')

with tabs[4]:
    section_title('Î”Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ­Ï‚ Î‘ÏƒÎºÎ®ÏƒÎµÎ¹Ï‚')
    
    st.markdown("""
    Î•Î´Ï Î¼Ï€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Ï€ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÏ„ÎµÎ¯Ï„Îµ Î¼Îµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î± ÎºÎ±Î¹ Î½Î± ÎµÏ†Î±ÏÎ¼ÏŒÏƒÎµÏ„Îµ Ï„Î¹Ï‚ Î³Î½ÏÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚!
    """)
    
    exercise_choice = st.selectbox('ğŸ¯ Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î†ÏƒÎºÎ·ÏƒÎ·:', [
        'Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼ÏÎ½ (Regression)',
        'Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î•Î¹ÎºÏŒÎ½Ï‰Î½ (Image Classification Simulation)',
        'Sentiment Analysis Simulator',
        'Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Î£Ï…ÏƒÏ„Î¬ÏƒÎµÏ‰Î½'
    ])
    
    if exercise_choice == 'Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼ÏÎ½ (Regression)':
        st.markdown('### ğŸ  Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼ÏÎ½ Î‘ÎºÎ¹Î½Î®Ï„Ï‰Î½')
        
        st.markdown("""
        Î£Îµ Î±Ï…Ï„Î® Ï„Î·Î½ Î¬ÏƒÎºÎ·ÏƒÎ· Î¸Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÏ„Îµ Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î¿ Ï€Î¿Ï… Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ Ï„Î·Î½ Ï„Î¹Î¼Î® ÎµÎ½ÏŒÏ‚ Î±ÎºÎ¹Î½Î®Ï„Î¿Ï… 
        Î²Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Ï„Î¿Ï….
        """)
        
        # Simulate housing data
        np.random.seed(42)
        n_houses = st.slider('Î ÏŒÏƒÎ± Î±ÎºÎ¯Î½Î·Ï„Î± ÏƒÏ„Î¿ dataset;', 50, 500, 200)
        
        size = np.random.randint(50, 300, n_houses)
        rooms = np.random.randint(1, 6, n_houses)
        age = np.random.randint(0, 50, n_houses)
        
        price = (size * 1000 + rooms * 15000 - age * 500 + 
                 np.random.randn(n_houses) * 10000)
        
        df_houses = pd.DataFrame({
            'ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ (Ï„.Î¼.)': size,
            'Î”Ï‰Î¼Î¬Ï„Î¹Î±': rooms,
            'Î—Î»Î¹ÎºÎ¯Î± (Î­Ï„Î·)': age,
            'Î¤Î¹Î¼Î® (â‚¬)': price
        })
        
        st.dataframe(df_houses.head(10), use_container_width=True)
        
        if st.button('ğŸš€ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· ÎœÎ¿Î½Ï„Î­Î»Î¿Ï… Î ÏÏŒÎ²Î»ÎµÏˆÎ·Ï‚'):
            from sklearn.linear_model import LinearRegression
            from sklearn.metrics import mean_absolute_error, r2_score
            
            X_houses = df_houses[['ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ (Ï„.Î¼.)', 'Î”Ï‰Î¼Î¬Ï„Î¹Î±', 'Î—Î»Î¹ÎºÎ¯Î± (Î­Ï„Î·)']].values
            y_houses = df_houses['Î¤Î¹Î¼Î® (â‚¬)'].values
            
            X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(
                X_houses, y_houses, test_size=0.3, random_state=42)
            
            model_h = LinearRegression().fit(X_train_h, y_train_h)
            y_pred_h = model_h.predict(X_test_h)
            
            mae = mean_absolute_error(y_test_h, y_pred_h)
            r2 = r2_score(y_test_h, y_pred_h)
            
            col_h1, col_h2 = st.columns(2)
            with col_h1:
                st.metric('ğŸ“Š RÂ² Score', f'{r2:.3f}')
            with col_h2:
                st.metric('ğŸ’° Mean Absolute Error', f'{mae:,.0f} â‚¬')
            
            fig_pred, ax_pred = plt.subplots(figsize=(10, 6))
            ax_pred.scatter(y_test_h, y_pred_h, alpha=0.6, edgecolors='k')
            ax_pred.plot([y_test_h.min(), y_test_h.max()], 
                        [y_test_h.min(), y_test_h.max()], 
                        'r--', lw=2, label='Perfect Prediction')
            ax_pred.set_xlabel('Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ® Î¤Î¹Î¼Î® (â‚¬)')
            ax_pred.set_ylabel('Î ÏÎ¿Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î¤Î¹Î¼Î® (â‚¬)')
            ax_pred.set_title('Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ­Ï‚ vs Î ÏÎ¿Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½ÎµÏ‚ Î¤Î¹Î¼Î­Ï‚')
            ax_pred.legend()
            ax_pred.grid(True, alpha=0.3)
            st.pyplot(fig_pred)
            plt.close()
            
            st.success('âœ… ÎœÎ¿Î½Ï„Î­Î»Î¿ ÎµÎºÏ€Î±Î¹Î´ÎµÏÏ„Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!')
            
            # Interactive prediction
            st.markdown('### ğŸ¡ Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ Î¼Î¹Î± Î ÏÏŒÎ²Î»ÎµÏˆÎ·')
            col_p1, col_p2, col_p3 = st.columns(3)
            with col_p1:
                new_size = st.number_input('ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ (Ï„.Î¼.)', 50, 300, 100)
            with col_p2:
                new_rooms = st.number_input('Î”Ï‰Î¼Î¬Ï„Î¹Î±', 1, 5, 3)
            with col_p3:
                new_age = st.number_input('Î—Î»Î¹ÎºÎ¯Î± (Î­Ï„Î·)', 0, 50, 10)
            
            if st.button('ğŸ”® Î ÏÏŒÎ²Î»ÎµÏˆÎ· Î¤Î¹Î¼Î®Ï‚'):
                new_pred = model_h.predict([[new_size, new_rooms, new_age]])[0]
                st.balloons()
                st.success(f'ğŸ  Î•ÎºÏ„Î¹Î¼ÏÎ¼ÎµÎ½Î· Î¤Î¹Î¼Î®: **{new_pred:,.0f} â‚¬**')
    
    elif exercise_choice == 'Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î•Î¹ÎºÏŒÎ½Ï‰Î½ (Image Classification Simulation)':
        st.markdown('### ğŸ“¸ Simulation: Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î•Î¹ÎºÏŒÎ½Ï‰Î½')
        
        st.markdown("""
        Î ÏÎ¿ÏƒÎ¿Î¼Î¿Î¯Ï‰ÏƒÎ· ÎµÎ½ÏŒÏ‚ image classifier. Î£Ï„Î·Î½ Ï€ÏÎ¬Î¾Î· Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÏƒÎ±Î¼Îµ CNN (Convolutional Neural Networks).
        """)
        
        st.info("""
        **Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Workflow:**
        1. Load dataset (Ï€.Ï‡. MNIST, CIFAR-10)
        2. Preprocess ÎµÎ¹ÎºÏŒÎ½ÎµÏ‚ (normalization, augmentation)
        3. Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± CNN architecture
        4. Training Î¼Îµ backpropagation
        5. Evaluation ÎºÎ±Î¹ fine-tuning
        """)
        
        # Simulate image classification
        categories = ['Î“Î¬Ï„Î±', 'Î£ÎºÏÎ»Î¿Ï‚', 'Î Î¿Ï…Î»Î¯', 'Î¨Î¬ÏÎ¹', 'Î‘Ï…Ï„Î¿ÎºÎ¯Î½Î·Ï„Î¿']
        
        num_images = st.slider('Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ ÎµÎ¹ÎºÏŒÎ½Ï‰Î½ ÏƒÏ„Î¿ dataset', 100, 1000, 500)
        
        # Simulate accuracy over epochs
        epochs = 20
        train_acc = []
        val_acc = []
        
        for epoch in range(epochs):
            train_acc.append(min(0.5 + epoch * 0.025 + np.random.rand() * 0.02, 0.98))
            val_acc.append(min(0.4 + epoch * 0.022 + np.random.rand() * 0.03, 0.92))
        
        fig_training, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        ax1.plot(range(1, epochs+1), train_acc, label='Training Accuracy', marker='o')
        ax1.plot(range(1, epochs+1), val_acc, label='Validation Accuracy', marker='s')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Training Progress')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Confusion matrix simulation
        cm_sim = np.random.randint(0, 50, (len(categories), len(categories)))
        np.fill_diagonal(cm_sim, np.random.randint(80, 120, len(categories)))
        
        sns.heatmap(cm_sim, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=categories, yticklabels=categories, ax=ax2)
        ax2.set_title('Confusion Matrix')
        ax2.set_xlabel('Predicted')
        ax2.set_ylabel('Actual')
        
        st.pyplot(fig_training)
        plt.close()
        
        st.success(f'âœ… Final Validation Accuracy: {val_acc[-1]:.2%}')
    
    elif exercise_choice == 'Sentiment Analysis Simulator':
        st.markdown('### ğŸ’¬ Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î¿Ï‚ (Sentiment Analysis)')
        
        st.markdown("""
        Î•Î¹ÏƒÎ¬Î³ÎµÏ„Îµ Î­Î½Î± ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÎºÎ±Î¹ Ï„Î¿ "Î¼Î¿Î½Ï„Î­Î»Î¿" Î¸Î± Ï€ÏÎ¿ÏƒÏ€Î±Î¸Î®ÏƒÎµÎ¹ Î½Î± ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÎµÎ¹ Ï„Î¿ ÏƒÏ…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±!
        """)
        
        user_text = st.text_area('âœï¸ Î“ÏÎ¬ÏˆÏ„Îµ Î­Î½Î± ÎºÎµÎ¯Î¼ÎµÎ½Î¿:', 
                                 'Î‘Ï…Ï„Î® Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® AI ÎµÎ¯Î½Î±Î¹ Ï†Î±Î½Ï„Î±ÏƒÏ„Î¹ÎºÎ®! ÎœÎ±Î¸Î±Î¯Î½Ï‰ Ï€Î¿Î»Î»Î¬!')
        
        if st.button('ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ· Î£Ï…Î½Î±Î¹ÏƒÎ¸Î®Î¼Î±Ï„Î¿Ï‚'):
            # Simple rule-based sentiment (simulation)
            positive_words = ['ÎºÎ±Î»ÏŒ', 'Ï†Î±Î½Ï„Î±ÏƒÏ„Î¹ÎºÏŒ', 'ÎµÎ¾Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ', 'Ï…Ï€Î­ÏÎ¿Ï‡Î¿', 'Î¬ÏÎ¹ÏƒÏ„Î¿', 
                            'Ï‡Î±ÏÎ¿ÏÎ¼ÎµÎ½Î¿Ï‚', 'Î¹ÎºÎ±Î½Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿Ï‚', 'Î¼Î±Î¸Î±Î¯Î½Ï‰', 'ÎµÏ…Ï‡Î±ÏÎ¹ÏƒÏ„Ï']
            negative_words = ['ÎºÎ±ÎºÏŒ', 'Î¬ÏƒÏ‡Î·Î¼Î¿', 'Î±Ï€Î±Î¯ÏƒÎ¹Î¿', 'Î´ÏÏƒÎºÎ¿Î»Î¿', 'Ï€ÏÏŒÎ²Î»Î·Î¼Î±',
                            'Î»Ï…Ï€Î·Î¼Î­Î½Î¿Ï‚', 'Î±Ï€Î¿Î³Î¿Î·Ï„ÎµÏ…Î¼Î­Î½Î¿Ï‚', 'Î»Î¬Î¸Î¿Ï‚']
            
            text_lower = user_text.lower()
            pos_count = sum(1 for word in positive_words if word in text_lower)
            neg_count = sum(1 for word in negative_words if word in text_lower)
            
            if pos_count > neg_count:
                sentiment = 'ğŸ˜Š Î˜ÎµÏ„Î¹ÎºÏŒ'
                color = 'green'
                score = min((pos_count / (pos_count + neg_count + 1)) * 100, 95)
            elif neg_count > pos_count:
                sentiment = 'ğŸ˜ Î‘ÏÎ½Î·Ï„Î¹ÎºÏŒ'
                color = 'red'
                score = min((neg_count / (pos_count + neg_count + 1)) * 100, 95)
            else:
                sentiment = 'ğŸ˜ ÎŸÏ…Î´Î­Ï„ÎµÏÎ¿'
                color = 'gray'
                score = 50
            
            col_s1, col_s2 = st.columns(2)
            with col_s1:
                st.markdown(f'### Î£Ï…Î½Î±Î¯ÏƒÎ¸Î·Î¼Î±: {sentiment}')
            with col_s2:
                st.metric('Î’Î±Î¸Î¼Î¿Î»Î¿Î³Î¯Î± Î’ÎµÎ²Î±Î¹ÏŒÏ„Î·Ï„Î±Ï‚', f'{score:.0f}%')
            
            st.progress(score / 100)
            
            st.info("""
            **Î£Î·Î¼ÎµÎ¯Ï‰ÏƒÎ·**: Î‘Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Î­Î½Î± Î±Ï€Î»Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±. Î ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ sentiment analysis 
            Î¼Î¿Î½Ï„Î­Î»Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½:
            - Deep Learning (LSTM, Transformers)
            - Pre-trained models (BERT, RoBERTa)
            - Context understanding
            - Multilingual support
            """)
    
    elif exercise_choice == 'Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î£Ï…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚ Î£Ï…ÏƒÏ„Î¬ÏƒÎµÏ‰Î½':
        st.markdown('### ğŸ¬ Î£ÏÏƒÏ„Î·Î¼Î± Î£Ï…ÏƒÏ„Î¬ÏƒÎµÏ‰Î½ (Recommendation System)')
        
        st.markdown("""
        Î ÏÎ¿ÏƒÎ¿Î¼Î¿Î¯Ï‰ÏƒÎ· ÎµÎ½ÏŒÏ‚ recommendation system Î³Î¹Î± Ï„Î±Î¹Î½Î¯ÎµÏ‚.
        """)
        
        # Sample movies
        movies = {
            'Action': ['Mad Max', 'John Wick', 'Die Hard', 'Terminator'],
            'Drama': ['The Godfather', 'Schindler\'s List', 'Forrest Gump'],
            'Comedy': ['The Hangover', 'Superbad', 'Bridesmaids'],
            'Sci-Fi': ['Inception', 'Interstellar', 'The Matrix', 'Blade Runner']
        }
        
        st.markdown('#### Î’Î®Î¼Î± 1: Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Ï„Î±Î¹Î½Î¯ÎµÏ‚ Ï€Î¿Ï… ÏƒÎ±Ï‚ Î±ÏÎ­ÏƒÎ¿Ï…Î½')
        
        liked_movies = []
        for genre, movie_list in movies.items():
            selected = st.multiselect(f'{genre}:', movie_list, key=genre)
            liked_movies.extend([(movie, genre) for movie in selected])
        
        if st.button('ğŸ¯ Î›Î®ÏˆÎ· Î£Ï…ÏƒÏ„Î¬ÏƒÎµÏ‰Î½') and liked_movies:
            st.markdown('#### ğŸ¬ Î ÏÎ¿Ï„ÎµÎ¹Î½ÏŒÎ¼ÎµÎ½ÎµÏ‚ Î¤Î±Î¹Î½Î¯ÎµÏ‚:')
            
            # Count genres
            from collections import Counter
            genres_count = Counter([genre for _, genre in liked_movies])
            top_genre = genres_count.most_common(1)[0][0] if genres_count else 'Action'
            
            # Recommend from top genre
            recommendations = [m for m in movies[top_genre] 
                             if m not in [movie for movie, _ in liked_movies]][:3]
            
            for rec in recommendations:
                st.success(f'âœ¨ {rec} ({top_genre})')
            
            st.info(f'ğŸ’¡ Î’Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿ ÏƒÏ„Î¹Ï‚ Ï€ÏÎ¿Ï„Î¹Î¼Î®ÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚ Î³Î¹Î± {top_genre}!')
            
            # Visualize preferences
            fig_pref, ax_pref = plt.subplots(figsize=(8, 5))
            genres = list(genres_count.keys())
            counts = list(genres_count.values())
            ax_pref.bar(genres, counts, color='skyblue', edgecolor='black')
            ax_pref.set_xlabel('Genre')
            ax_pref.set_ylabel('Number of Liked Movies')
            ax_pref.set_title('Your Genre Preferences')
            st.pyplot(fig_pref)
            plt.close()

with tabs[5]:
    section_title('Î ÏŒÏÎ¿Î¹ & ÎŸÎ´Î·Î³Î¯ÎµÏ‚')
    
    st.markdown("""
    ## ğŸ“š Î ÏÏŒÏƒÎ¸ÎµÏ„Î¿Î¹ Î ÏŒÏÎ¿Î¹ Î³Î¹Î± ÎœÎ¬Î¸Î·ÏƒÎ·
    
    ### ğŸŒ Online Courses
    - **Coursera**: Machine Learning by Andrew Ng
    - **Fast.ai**: Practical Deep Learning
    - **DeepLearning.AI**: Deep Learning Specialization
    - **Udacity**: AI Programming with Python
    
    ### ğŸ“– Î’Î¹Î²Î»Î¯Î±
    - "Hands-On Machine Learning" - AurÃ©lien GÃ©ron
    - "Deep Learning" - Ian Goodfellow
    - "Pattern Recognition and Machine Learning" - Christopher Bishop
    - "AI: A Modern Approach" - Russell & Norvig
    
    ### ğŸ’» Frameworks & Tools
    - **TensorFlow**: Deep learning framework by Google
    - **PyTorch**: Deep learning framework by Meta
    - **Scikit-learn**: Machine learning library
    - **Keras**: High-level neural networks API
    - **Hugging Face**: Pre-trained models
    
    ### ğŸ“ Î Î±Î½ÎµÏ€Î¹ÏƒÏ„Î·Î¼Î¹Î±ÎºÎ¬ ÎœÎ±Î¸Î®Î¼Î±Ï„Î± (Free)
    - MIT OpenCourseWare: Introduction to AI
    - Stanford CS229: Machine Learning
    - Berkeley CS188: Introduction to AI
    
    ### ğŸ¤ Communities
    - **Kaggle**: Competitions & Datasets
    - **GitHub**: Open source projects
    - **Stack Overflow**: Q&A
    - **Reddit**: r/MachineLearning, r/artificial
    
    ### ğŸ“Š Datasets
    - **UCI Machine Learning Repository**
    - **Kaggle Datasets**
    - **Google Dataset Search**
    - **ImageNet**
    - **COCO Dataset**
    
    ---
    
    ## ğŸš€ ÎŸÎ´Î·Î³Î¯ÎµÏ‚ Î§ÏÎ®ÏƒÎ·Ï‚ Î•Ï†Î±ÏÎ¼Î¿Î³Î®Ï‚
    
    1. **Î ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿**: Î”Î¹Î±Î²Î¬ÏƒÏ„Îµ Ï„Î· Î¸ÎµÏ‰ÏÎ¯Î± Î³Î¹Î± Ï„Î¹Ï‚ ÎµÎ½ÏŒÏ„Î·Ï„ÎµÏ‚ 1.1-1.7
    2. **Î Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Python**: Î ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÏƒÏ„ÎµÎ¯Ï„Îµ Î¼Îµ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± ML
    3. **Î•Î¾Î¿Î¼Î¿Î¹ÏÏƒÎµÎ¹Ï‚**: Î”ÎµÎ¯Ï„Îµ Ï€ÏÏ‚ Î¿Î¹ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ ÎµÏ€Î·ÏÎµÎ¬Î¶Î¿Ï…Î½ Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î±
    4. **ÎšÎ¿Ï…Î¯Î¶**: Î•Î»Î­Î³Î¾Ï„Îµ Ï„Î¹Ï‚ Î³Î½ÏÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚
    5. **Î”Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ­Ï‚ Î‘ÏƒÎºÎ®ÏƒÎµÎ¹Ï‚**: Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Îµ Ï„Î¹Ï‚ Î³Î½ÏÏƒÎµÎ¹Ï‚ ÏƒÎ±Ï‚ ÏƒÎµ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ¬ Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î±
    
    ---
    
    ## ğŸ“ Î“Î¹Î± Offline Î§ÏÎ®ÏƒÎ·
    
    Î‘Ï…Ï„Î® Î· ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï„ÏÎ­Î¾ÎµÎ¹ ÎµÎ½Ï„ÎµÎ»ÏÏ‚ offline. Î“Î¹Î± Î½Î± Ï„Î·Î½ ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÎµÏ„Îµ:
    
    ```bash
    # Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· dependencies
    pip install streamlit scikit-learn matplotlib numpy pandas seaborn
    
    # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÎµÏ†Î±ÏÎ¼Î¿Î³Î®Ï‚
    streamlit run ai_training_app.py
    ```
    
    ---
    
    ## ğŸ¯ Î•Ï€ÏŒÎ¼ÎµÎ½Î± Î’Î®Î¼Î±Ï„Î±
    
    1. ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÏƒÏ„Îµ ÏŒÎ»Î± Ï„Î± ÎºÎ¿Ï…Î¯Î¶
    2. Î”Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Îµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÎµÎ¾Î¿Î¼Î¿Î¹ÏÏƒÎµÎ¹Ï‚
    3. ÎšÎ¬Î½Ï„Îµ Ï„Î¹Ï‚ Î´Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÎ­Ï‚ Î±ÏƒÎºÎ®ÏƒÎµÎ¹Ï‚
    4. Î•Î¾ÎµÏÎµÏ…Î½Î®ÏƒÏ„Îµ Ï„Î¿Ï…Ï‚ Ï€ÏÎ¿Ï„ÎµÎ¹Î½ÏŒÎ¼ÎµÎ½Î¿Ï…Ï‚ Ï€ÏŒÏÎ¿Ï…Ï‚
    5. ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ±Ï‚ AI project!
    
    ---
    
    ## ğŸ“§ Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î± & Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î·
    
    Î“Î¹Î± ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ ÎºÎ±Î¹ Ï…Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î·, Î±Î½Î±Ï„ÏÎ­Î¾Ï„Îµ ÏƒÏ„Î¿ ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒ Ï…Î»Î¹ÎºÏŒ ÎºÎ±Î¹ Ï„Î¿Ï…Ï‚ Ï€ÏŒÏÎ¿Ï…Ï‚ Ï€Î¿Ï… Ï€Î±ÏÎ­Ï‡Î¿Î½Ï„Î±Î¹.
    
    **ÎšÎ±Î»Î® ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î± ÏƒÏ„Î·Î½ ÎµÎºÎ¼Î¬Î¸Î·ÏƒÎ· AI!** ğŸš€ğŸ¤–
    """)
    
    # Footer
    st.markdown('---')
    st.markdown("""
    <div style='text-align: center; color: gray; padding: 20px;'>
        <p>ğŸ¤– AI Training Application v2.0</p>
        <p>Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ Î¼Îµ â¤ï¸ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Streamlit</p>
        <p>Î’Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î¿ ÏƒÏ„Î¿ ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒ Ï…Î»Î¹ÎºÏŒ: "Î•Ï†Î±ÏÎ¼Î¿Î³Î­Ï‚ Î¤ÎµÏ‡Î½Î·Ï„Î®Ï‚ ÎÎ¿Î·Î¼Î¿ÏƒÏÎ½Î·Ï‚ ÎºÎ±Î¹ ChatGPT ÏƒÎµ ÎšÏÎ¯ÏƒÎ¹Î¼Î¿Ï…Ï‚ Î¤Î¿Î¼ÎµÎ¯Ï‚"</p>
    </div>
    """, unsafe_allow_html=True)
